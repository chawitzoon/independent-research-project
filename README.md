# Monocular Depth Estimation via Transfer Learning and Multi-Task Learning with Semantic Segmentation
Term project in Bachelor program of Tokyo Institute of Technology, Transdisciplinary Science and Engineering. Yamashita Laboratory. April - July 2019.

![result](https://github.com/chawitzoon/independent-research-project/assets/29890414/efebcbb7-4dee-4ae7-afd1-304a43c47ad6)

## abstract
Depth estimation from a single RGB image is recently applied in computer vision and scene understanding. Several existing depth estimation approaches are based on either stereo geometry or high energy-consuming active measurement, whereas, this study focuses on the machine learning approach using a convolutional neural network on monocular depth estimation. The proposed network architecture is an encoder-decoder network architecture with multi-task learning and transfer learning. The level of transfer learning, along with the application of multi-task learning with semantic segmentation, was studied. Quantitative and qualitative results show that the training with fine-tuning transfer learning along with multi-task learning with semantic segmentation gives a good prediction result with more stable sensitivity. The adjustment on the implementation and training, involving trained data and image augmentation, is potentially attractive as a future study.

## Network Architecture and Dataset
This study pioneered in leveraging two concepts (transfer learning and multi-task learning) for depth estimation in the same network architecture. There are studies about multi-task transfer (train on many tasks, transfer to a new task), but there is no obvious study that use a transferred network to learn more than one task at the same time. 

![network](https://github.com/chawitzoon/independent-research-project/assets/29890414/5f2b7f08-ffa8-4c43-8116-2b41abd6d7dd)

### Loss function
For depth estimation, the custom loss which combines three loss metrics is used. For depth estimation, the custom loss which combines three loss metrics is used

- $L_{L1}$: the point-wise L1 loss of depth values of the target ground-truth depth map and the
predicted depth map
$$L_{\text{L1}} = \frac{1}{n} \Sigma^n_p|y_p - \hat{y_p}|$$

- $L_{gradient}$: the point-wise L1 loss of the gradient values g of the target ground-truth depth
map and the predicted depth map
$$L_{\text{gradient}} = \frac{1}{n} \Sigma^n_p(|g_x(y_p) - g_x(\hat{y_p})| + |g_y(y_p) - g_y(\hat{y_p})|)$$

- $L_{\text{SSIM}}$:  the loss bases on Structural Similarity (SSIM), which is widely used to compare the similarity of two images.
$$L_{\text{SSIM}} = \frac{1 - \text{SSIM}(y, \hat{y})}{2}$$


The combined loss $L$ is the weighted sum of the losses. the weight parameters are arbitrarily set as follows,
$$L(y, \hat{y}) = 0.1L_{\text{L1}} + L_{\text{gradient}} + L_{\text{SSIM}}.$$

For multi-task learning with semantic segmentation, the previously mentioned custom loss is used for depth estimation task and the standard softmax cross-entropy loss is used for semantic segmentation task with the ratio of 1:0.2 respectively.

### Dataset
NYU Depth v2 dataset is a dataset comprised of RGB images at a resolution of 480 × 640 and their depth maps at a resolution of 240 × 320 for different indoor scenes. The dataset provides two sets of data; labeled dataset which includes 1449 images along with their ground truth depth maps, semantic segmentation and other parameters for the images, and raw data taken from depth and RGB cameras. Since the operation frequencies of these two cameras are different, the matching process is needed. The processing of matching along with inpainting for the pixels with no depth value is done by using the Matlab toolkit provided by the author of NYU Depth v2 dataset. In this investigation, 12564 raw data images were used as the train and the validation set, and all 1449 labeled data images were used as the test set. Semantic segmentation data for raw NYU Depth v2 images are generated by using a teacher network which has pre-trained weights on ADE20K dataset with 151 classes including indoor objects and scenes.

### Result
This study investigated six training cases from the combination of three levels of transfer learning (freezing the pre-trained parameters on ImageNet, fine-tuning the pre-trained weights, and random initialization of the model weights) and the application of multi-task learning (applying and not applying). Each training case is evaluated by two image sets; the test images dataset in NYU Depth v2 dataset, and real-world scenes. The training cases are described in the following list.
- Train1: the network with freezing pre-trained parameters in the encoder, not applying multi-task learning
- Train2: the network with freezing pre-trained parameters in the encoder, applying multi-task learning
- Train3: the network with fine-tuned parameters in the encoder, not applying multi-task learning
- Train4: the network with fine-tuned parameters in the encoder, applying multi-task learning
- Train5: the network with random initialized parameters in the encoder, not applying multi-task learning
- Train6: the network with random initialized parameters in the encoder, applying multi-task learning

![result2](https://github.com/chawitzoon/independent-research-project/assets/29890414/f4a09126-3320-49f8-9066-ec681601ca53)

Threshold accuracy ($\rho_i$): percentage of pixels ($(y_p)$) such that ($\rho < thr$) where $\rho = max(\frac{y_p}{\hat{y_p}},\frac{\hat{y_p}}{y_p})$ and $thr = 1.25, 1.25^2, 1.25^3$. Here, ($y_p$) is a pixel in target ground-truth depth image, and $(\hat{y_p})$ is a pixel in the predicted depth image.

### Conclusion
The study focused on the machine learning approach to find a depth value. The proposed network is an encoder-decoder network architecture for monocular depth estimation with multi-task learning and transfer learning. The experiment showed a good result of the prediction, especially, the training by fine-tuning transfer learning along with multi-task learning with semantic segmentation. The study illustrated the benefits of transfer learning, which leverages the knowledge from pre-trained parameters, and multi-task learning, which provides regularization and gives more stable sensitivity. Both quantitative and qualitative results indicate the improvement of the depth estimation task. Even though the proposed network does not outperform the evaluation metrics compared with state-of-the-art methods, small adjustments on the implementation and training, including trained data, augmentation, and hyperparameter setting, would increase the accuracy. 

### Future work
The future work could focus more on the efficiency and the performance of the training and predicting speed as further study. In the aspects of architecture, and parameter, it is believed that there is a relatively large space for improvement. Also, it is interesting to improve the network for real-time prediction to suit most recent real-world applications.

Furthermore, the future work could focus on implementing a larger dataset with the better data augmentation scheme. In this study, only 12K training images with an only horizontal flip as the augmentation was used due to some limitations such as time and resource. This is believed as the leading cause of overfitting and less evaluation score on the qualitative metrics than the state-of-the-art architecture.
